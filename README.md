# data_analytics_portfolio_project
This project is a Data Analytics case study built using Python and SQL, analyzing real e-commerce sales data from Brazil between 2016 and 2018. The dataset contains detailed information on customers, orders, payments, products, and customer locations, providing a complete view of the sales funnel.
The project covers data cleaning, SQL-based analysis, and Python-based exploratory analytics and visualizations to uncover patterns in customer behavior, sales performance, payment trends, and regional demand.
Insights were generated by combining SQL queries with Python data processing.

Tech Stack Used

Python (data cleaning, EDA, visualization)

Common libs: pandas, numpy, matplotlib/seaborn, sqlalchemy (optional)

SQL (analysis queries: joins, aggregations, segmentation)

Database: any SQL DB you used (commonly PostgreSQL / MySQL / SQLite)

Jupyter Notebook / VS Code (for running Python analysis)

Steps to Run the Project Locally

Download / place the dataset

Put all CSVs in a folder like: data/raw/

Create a Python environment & install dependencies

python -m venv venv
# Windows: venv\Scripts\activate
# Mac/Linux: source venv/bin/activate
pip install pandas numpy matplotlib seaborn sqlalchemy jupyter


Set up the SQL database (choose one)

Create a database (ex: brazil_ecommerce)

Import CSVs into tables like: customers, orders, payments, products, order_items, sellers, geolocation

(Optional) Put table creation scripts in sql/schema.sql and import scripts in sql/load_data.sql

Run SQL analysis

Execute queries from a folder like sql/analysis_queries.sql in your DB tool (pgAdmin / MySQL Workbench / sqlite browser).

Run Python notebooks / scripts

Start Jupyter:

jupyter notebook


Open notebooks (example):

notebooks/01_data_cleaning.ipynb

notebooks/02_exploratory_analysis.ipynb

notebooks/03_visualizations.ipynb

Python will either:

read cleaned CSVs, or

connect to DB (via SQLAlchemy) and pull query results.

Assumptions / Limitations

Time range limited (2016–2018) → trends may not reflect current customer behavior.

Data quality depends on source → missing values, inconsistent city/state naming, and duplicates may exist.
